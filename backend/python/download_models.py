#!/usr/bin/env python3
"""
Pre-download ML models to avoid downloading during runtime
Run this once after installing requirements.txt
"""

import os
from sentence_transformers import SentenceTransformer

def download_models():
    """Download and cache the required models"""
    print("ğŸ“¥ Downloading ML models for caching...")
    
    # Download the main model used by the application
    model_name = 'all-mpnet-base-v2'
    print(f"Downloading {model_name}...")
    
    try:
        # This will download and cache the model
        model = SentenceTransformer(model_name)
        print(f"âœ… Successfully downloaded and cached {model_name}")
        
        # Test the model to ensure it works
        test_texts = ["This is a test sentence.", "This is another test."]
        embeddings = model.encode(test_texts)
        print(f"âœ… Model test successful - generated embeddings of shape: {embeddings.shape}")
        
        # Show cache location
        import torch
        cache_dir = torch.hub.get_dir()
        print(f"ğŸ“ Models cached in Hugging Face cache directory")
        
        # Test offline loading
        print("\nğŸ”’ Testing offline configuration...")
        from offline_config import setup_offline_environment
        setup_offline_environment()
        
        # Try loading model again in offline mode
        model_offline = SentenceTransformer(model_name)
        test_embeddings = model_offline.encode(["Offline test"])
        print(f"âœ… Offline mode works - embeddings shape: {test_embeddings.shape}")
        
    except Exception as e:
        print(f"âŒ Error downloading model: {e}")
        return False
    
    print("ğŸ‰ All models downloaded successfully!")
    print("ğŸ’¡ Your Flask server will now start much faster!")
    print("ğŸ”’ Offline mode configured to prevent network timeouts!")
    return True

if __name__ == "__main__":
    download_models()